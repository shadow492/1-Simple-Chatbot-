# -*- coding: utf-8 -*-
"""chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IbhYQfmxpUXtxlhWMfRYBdtKaHm-83vA
"""

import random
import nltk
from nltk.stem.lancaster import LancasterStemmer
stemmer = LancasterStemmer()

import nltk
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
import json
nltk.download('punkt')

# Initialize stemmer
stemmer = PorterStemmer()

words = []
classes = []
documents = []
ignore_words = ['?']

# Load intents data
with open('/content/intents.json', 'r') as file:
    intents = json.load(file)

# Process each intent in the intents file
for intent in intents['intents']:
    for pattern in intent['patterns']:
        # Tokenize the pattern into individual words
        tokenized_words = word_tokenize(pattern)

        # Extend the words list with tokenized words
        words.extend(tokenized_words)

        # Append the tokenized words and intent tag to documents
        documents.append((tokenized_words, intent['tag']))

        # Add the intent tag to classes if not already present
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

# Stem, lowercase, and remove duplicates from the words list
words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]
words = sorted(set(words))

# Sort and remove duplicates from classes
classes = sorted(set(classes))

print(f"{len(documents)} documents")
print(f"{len(classes)} classes", classes)
print(f"{len(words)} unique stemmed words", words)

# Initialize training data lists
training_data = []
output_data = []
# Create an empty list for output tags
empty_output = [0] * len(classes)

# Process each document to create training data
for doc in documents:
    # Initialize bag of words
    bag_of_words = []
    # Tokenized words for the pattern
    tokenized_pattern = doc[0]
    # Stem and lowercase each word
    stemmed_words = [stemmer.stem(word.lower()) for word in tokenized_pattern]
    # Create bag of words array
    for word in words:
        bag_of_words.append(1 if word in stemmed_words else 0)

    # Create output row with '0' for each tag and '1' for the current tag
    output_row = empty_output[:]
    output_row[classes.index(doc[1])] = 1

    training_data.append([bag_of_words, output_row])

# Shuffle the training data and convert to numpy array
random.shuffle(training_data)
training_data = np.array(training_data, dtype=object)

# Split into train and test data
train_x = list(training_data[:, 0])
train_y = list(training_data[:, 1])

print(f"Training data created with {len(train_x)} patterns")

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Dropout
from tensorflow.keras.optimizers import SGD
import numpy as np

# Reset the underlying graph data
tf.keras.backend.clear_session()

# Build the neural network
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(1288, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Compile the model
sgd = SGD(learning_rate=0.01, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

# Train the model
model.fit(np.array(train_x), np.array(train_y), epochs=1000, batch_size=8, verbose=1)

# Save the model
model.save('model.h5')

from nltk.stem import WordNetLemmatizer

import json
import numpy as np
import random
import nltk
from tensorflow.keras.models import load_model
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# Load intents JSON
with open('intents.json', 'r') as file:
    intents = json.load(file)

# Initialize the stemmer
stemmer = PorterStemmer()

# Load the trained model
model = load_model('model.h5')

# Sample words and classes lists
words_list = words
classes_list = ['books', 'dreams', 'farewell', 'future_membership', 'greeting', 'hobbies', 'membership', 'motivational_quote', 'music', 'programming_languages', 'technology', 'thanks', 'today_membership', 'travel']

user_context = {}

# Preprocess user input
def preprocess_input(sentence):
    tokenized_words = word_tokenize(sentence)
    stemmed_words = [stemmer.stem(word.lower()) for word in tokenized_words]
    return stemmed_words

# Convert input to bag-of-words representation
def input_to_bow(sentence, words_list, show_details=True):
    sentence_words = preprocess_input(sentence)
    bow = [0] * len(words_list)
    for s in sentence_words:
        for i, word in enumerate(words_list):
            if word == s:
                bow[i] = 1
                if show_details:
                    print(f"Found in words list: {word}")
    return np.array(bow)

# Predict the intent
def predict_intent(sentence, model):
    bow = input_to_bow(sentence, words_list, show_details=False)
    bow = bow.reshape(1, -1)  # Reshape for model input
    prediction = model.predict(bow)[0]
    THRESHOLD = 0.25
    results = [[i, prob] for i, prob in enumerate(prediction) if prob > THRESHOLD]

    results.sort(key=lambda x: x[1], reverse=True)
    return [{"intent": classes_list[r[0]], "probability": str(r[1])} for r in results]

# Get response considering context
def get_response(predicted_intents, intents_json, user_id='123'):
    if not predicted_intents:
        return "I'm not sure I understand."

    for intent in intents_json['intents']:
        if intent['tag'] == predicted_intents[0]['intent']:
            if 'context_set' in intent:
                user_context[user_id] = intent['context_set']

            if 'context_filter' not in intent or (user_id in user_context and intent['context_filter'] == user_context[user_id]):
                return random.choice(intent['responses'])

    return "I'm not sure I understand."

# Generate chatbot response
def generate_response(user_input, user_id='123'):
    predicted_intents = predict_intent(user_input, model)
    response = get_response(predicted_intents, intents, user_id)
    return response

# Interactive Chatbot
print("Chatbot is active! Type 'exit' to end the session.")
while True:
    user_message = input("You: ")
    if user_message.lower() == "exit":
        print("Bot: Goodbye!")
        break
    bot_response = generate_response(user_message)
    print(f"Bot: {bot_response}")

words

import pickle
pickle.dump({'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y},open("training_data.pickle","wb"))

